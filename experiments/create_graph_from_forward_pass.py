# Alternative method of selecting a sub-graph for visualization
# - Run model and SAEs on an input of 128 tokens
# - Pick all features that are active on the last token
# - Pick the highest similarity predecessors
# - Plot the subgraph generated by these nodes

# %%
# Imports
import os
from jaxtyping import Float, Int
from torch import Tensor
import torch
import einops
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.colors import LinearSegmentedColormap
import networkx as nx
import pickle

import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))

from pipeline_helpers import load_model_and_saes, load_data
from similarity_helpers import get_filename, load_similarity_data
from explanation_helpers import add_explanations
from visualization import show_explanation_graph


# %%
device = "cuda:2"


# %%
sae_name = "res_jb_sae"

model, saes = load_model_and_saes(
    model_name="gpt2-small",
    sae_name="gpt2-small-res-jb",
    hook_name="hook_resid_pre",
    device=device,
)

tokens = load_data(
    model, saes[0], dataset_name="NeelNanda/pile-10k", number_of_batches=1
)
tokens = tokens[0:1]  # Only pick one row of the batch


# %%
# Run model and collect SAE activations
context_size = saes[0].cfg.context_size
d_sae = saes[0].cfg.d_sae
sae_activations = torch.empty(model.cfg.n_layers, d_sae, context_size)


def retrieval_hook(activations, hook):
    layer = hook.layer()

    sae_activations[layer] = einops.rearrange(
        # Get SAE activations
        saes[layer].encode(activations),
        "batch seq features -> features (batch seq)",
    )


model.add_hook(lambda name: name.endswith(".hook_resid_pre"), retrieval_hook)

with torch.no_grad():
    model.run_with_hooks(tokens)

# Select all features that are active (i.e., non-zero) on the last token
selection_threshold = 0.0
active_features = sae_activations[:, :, -1] > selection_threshold


# %%
# Build graph
graph = nx.DiGraph()

# Get nodes from activations
nonzero_features = [tuple(idx) for idx in active_features.nonzero().tolist()]
graph.add_nodes_from(
    [
        (f"{layer}_{feature}", {"layer": layer, "feature": feature})
        for layer, feature in nonzero_features
    ]
)

# Add explanations to nodes
with open(f"../artefacts/explanations/{sae_name}_explanations.pkl", "rb") as f:
    explanations = pickle.load(f)
for node, attr in graph.nodes(data=True):
    graph.nodes[node]["explanation"] = explanations[attr["layer"]][attr["feature"]]

# Add attributes to graph
graph.graph["description"] = (
    f"This graph's nodes are the SAE features that are active (i.e., whose activation is {selection_threshold} or higher) on the final token of the prompt. Its edges represent the similarity values of the {measure_name} measure, computed over {n_tokens} tokens with activation threshold {activation_threshold} (absolute values below {clamping_threshold} are clamped to zero). The explanations of the features are created by GPT-3.5-turbo and downloaded from Neuronpedia."
)
graph.graph["prompt"] = model.to_string(tokens)
graph.graph["clamping_threshold"] = clamping_threshold

# Get edges from similarity measure
measure_name = "cosine_similarity"
n_layers = 12
activation_threshold = None
n_tokens = None
clamping_threshold = 0.4

folder = f"../artefacts/similarity_measures/{measure_name}"

similarities = load_similarity_data(
    [
        f'{folder}/{get_filename(measure_name, "feature_similarity", activation_threshold, clamping_threshold=clamping_threshold, n_tokens=n_tokens, first_layer=None, sae_name=sae_name)}.npz'
    ]
)


# %%
nonzero_features_per_layer = [[] for _ in range(model.cfg.n_layers)]
for layer, feature in nonzero_features:
    nonzero_features_per_layer[layer].append(feature)

for layer, upstream_features, downstream_features in zip(
    range(model.cfg.n_layers),
    nonzero_features_per_layer,
    nonzero_features_per_layer[1:],
):
    graph.add_edges_from(
        [
            (
                f"{layer}_{up_feat}",
                f"{layer+1}_{down_feat}",
                {[measure_name]: similarities[layer, up_feat, down_feat]},
            )
            for up_feat in upstream_features
            for down_feat in downstream_features
        ]
    )


# %%
show_explanation_graph(graph)


# %%
nonzero_features_per_layer