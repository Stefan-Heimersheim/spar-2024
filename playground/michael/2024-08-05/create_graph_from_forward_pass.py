# Alternative method of selecting a sub-graph for visualization
# - Run model and SAEs on an input of 128 tokens
# - Pick all features that are active on the last token
# - Pick the highest similarity predecessors
# - Plot the subgraph generated by these nodes

# %%
# Imports
import os
# OPTIONAL: Set environment variable to control visibility of GPUs
os.environ["CUDA_VISIBLE_DEVICES"] = "7"

from jaxtyping import Float, Int
from torch import Tensor
import torch
import einops
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.colors import LinearSegmentedColormap
import networkx as nx

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..', 'src'))

from pipeline_helpers import load_model_and_saes, load_data
from explanation_helpers import add_explanations
from visualization import show_explanation_graph


# %%
# OPTIONAL: Check if the correct GPU is visible
print(torch.cuda.device_count())  # Should print 1
print(torch.cuda.current_device())  # Should print 0 since it's the first visible device
print(torch.cuda.get_device_name(0))  # Should print the name of the GPU

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")


# %%
model, saes = load_model_and_saes(model_name='gpt2-small', sae_name='gpt2-small-res-jb', hook_name='hook_resid_pre', device=device)

tokens = load_data(model, saes[0], dataset_name='NeelNanda/pile-10k', number_of_batches=1)
tokens = tokens[0:1]  # Only pick one row of the batch


# %%
# Run model and collect SAE activations
context_size = saes[0].cfg.context_size
d_sae = saes[0].cfg.d_sae
sae_activations = torch.empty(model.cfg.n_layers, d_sae, context_size)


def retrieval_hook(activations, hook):
    layer = hook.layer()

    sae_activations[layer] = einops.rearrange(
        # Get SAE activations
        saes[layer].encode(activations), "batch seq features -> features (batch seq)"
    )


model.add_hook(lambda name: name.endswith(".hook_resid_pre"), retrieval_hook)

with torch.no_grad():
    model.run_with_hooks(tokens)

active_features = sae_activations[:, :, -1] > 0


# %%
# Build graph
graph = nx.DiGraph()

# Get nodes from activations
nonzero_features = [tuple(idx) for idx in active_features.nonzero().tolist()]

graph.add_nodes_from([(f'{layer}_{feature}', {'layer': layer, 'feature': feature}) for layer, feature in nonzero_features])
add_explanations(graph, max_workers=10)


# %%
# Get edges from similarity measure
artefacts_folder = f'../../artefacts/similarity_measures'
sae_name = 'res_jb_sae'
measure = 'pearson_correlation'
clamping_threshold = 0.1
token_desc = '1M'
activation_threshold = 0.0
filename = f'{sae_name}_feature_similarity_{measure}_{token_desc}_{activation_threshold:.1f}_{clamping_threshold}'

similarities = np.load(f'{artefacts_folder}/{measure}/{filename}.npz')['arr_0']


# %%
nonzero_features_per_layer = [[] for _ in range(model.cfg.n_layers)]
for layer, feature in nonzero_features:
    nonzero_features_per_layer[layer].append(feature)

for layer, upstream_features, downstream_features in zip(range(model.cfg.n_layers), nonzero_features_per_layer, nonzero_features_per_layer[1:]):
    print(len(upstream_features), len(downstream_features))
    graph.add_edges_from([(f'{layer}_{up_feat}', f'{layer+1}_{down_feat}', {'similarity': similarities[layer, up_feat, down_feat], 'measure': measure}) for up_feat in upstream_features for down_feat in downstream_features])


# %%
show_explanation_graph(graph)
