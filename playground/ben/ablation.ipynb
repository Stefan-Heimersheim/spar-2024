{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm import tqdm, trange\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "import jaxtyping as jt\n",
    "\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clamping SAE features in one layer and measuring effects on the subsequent layer\n",
    "## Plan described [here](https://spar2024.slack.com/archives/C0794GNT8KS/p1719950740186749?thread_ts=1719934219.491869&cid=C0794GNT8KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small set of correlations to play around with\n",
    "pearson_0_1_small: 'f' = t.load('../../data/res_jb_sae_feature_correlation_pearson_0_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest correlations\n",
    "def create_value_tensor(matrix: 'f,f') -> 'f*f,3':\n",
    "    m, _ = matrix.shape\n",
    "    \n",
    "    # Step 1: Flatten the matrix (shape: [m*m])\n",
    "    flattened_matrix = matrix.flatten()\n",
    "    \n",
    "    # Step 2: Create row and column indices\n",
    "    row_indices = t.arange(m).repeat_interleave(m)\n",
    "    col_indices = t.arange(m).repeat(m)\n",
    "    \n",
    "    # Step 3: Create the final tensor with indices and values\n",
    "    values = flattened_matrix\n",
    "    result = t.stack((row_indices, col_indices, values), dim=1)\n",
    "    \n",
    "    # Step 4: Sort the result tensor by values\n",
    "    sorted_result = result[t.argsort(result[:, 2], descending=True)]\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = create_value_tensor(pearson_0_1_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0300e+03,  2.3056e+04,         nan],\n",
       "        [ 1.1956e+04,  1.6733e+04,         nan],\n",
       "        [ 2.4575e+04,  2.4572e+04,         nan],\n",
       "        ...,\n",
       "        [ 1.3725e+04,  9.4870e+03, -1.4345e-01],\n",
       "        [ 1.3725e+04,  1.5370e+04, -1.4875e-01],\n",
       "        [ 1.3725e+04,  2.2119e+04, -1.5495e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to identify rows where the last column is not NaN\n",
    "not_nan_mask = ~torch.isnan(ranked_features[:, 2])\n",
    "\n",
    "# Create a mask to identify rows where the last column is less than 1\n",
    "less_than_one_mask = ranked_features[:, 2] < 0.99\n",
    "\n",
    "# Combine masks using logical AND\n",
    "combined_mask = not_nan_mask & less_than_one_mask\n",
    "\n",
    "# Use the combined mask to filter the rows\n",
    "filtered_tensor = ranked_features[combined_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0715e+04,  2.0175e+04,  9.8999e-01],\n",
       "        [ 1.8389e+04,  5.4670e+03,  9.8998e-01],\n",
       "        [ 6.2600e+03,  1.3641e+04,  9.8998e-01],\n",
       "        ...,\n",
       "        [ 1.3725e+04,  9.4870e+03, -1.4345e-01],\n",
       "        [ 1.3725e+04,  1.5370e+04, -1.4875e-01],\n",
       "        [ 1.3725e+04,  2.2119e+04, -1.5495e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9900)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_0_1_small[10715, 20175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dani:\n",
    "probably a way to get the highest values builtin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the correlation when we pass the residual stream that's reconstructed from the SAE features - NO clamping\n",
    "As mentioned at the end of June, normally the SAE features values are read by us and discarded - they are not passed back into the model for inference.\n",
    "However, if we are going to be clamping an SAE feature and seeing its impact downstream, then we need to first see what happens when we pass the SAE features downstream with no clamping - because the mere act of projecting a residual stream into an SAE space and then back into residual stream space is a lossy operation (even though the SAE is supposed to represent the residual stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benlerner/work/spar-2024/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 12/12 [00:07<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae_id_to_sae = {}\n",
    "for layer in tqdm(list(range(model.cfg.n_layers))):\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=sae_id,\n",
    "        device=device\n",
    "    )\n",
    "    sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "    sae_id_to_sae[sae_id] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# These hyperparameters are used to pre-process the data\n",
    "pre_0_sae_id = \"blocks.0.hook_resid_pre\"\n",
    "pre_0_sae = sae_id_to_sae[pre_0_sae_id]\n",
    "context_size = pre_0_sae.cfg.context_size\n",
    "prepend_bos = pre_0_sae.cfg.prepend_bos\n",
    "d_sae = pre_0_sae.cfg.d_sae\n",
    "batch_size = 32\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=context_size,\n",
    "    add_bos_token=prepend_bos,\n",
    ")\n",
    "\n",
    "tokens = token_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Reduce dataset for faster experimentation\n",
    "num_of_sentences = 1024\n",
    "tokens = tokens[:num_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,  1026,   318,  ...,   644,  1611,   286],\n",
       "        [50256,   983,   314,  ...,   983,    11,   345],\n",
       "        [50256,   761,   284,  ...,   765,   284,  2251],\n",
       "        ...,\n",
       "        [50256, 17151,  9936,  ...,   198,  7036,  1565],\n",
       "        [50256, 23715,    55,  ...,   257, 39733,   290],\n",
       "        [50256,  5680,   291,  ..., 41756,   287,   477]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tokens, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at compute-pearson-0.py to figure out how hooks work\n",
    "\n",
    "\n",
    "# okay so add_hook takes a function...and then...\n",
    "# https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.hook_points.html#transformer_lens.hook_points.HookPoint.add_hook\n",
    "\n",
    "# TODO: make a lambda function\n",
    "# model.reset_hooks()\n",
    "\n",
    "# TODO: find out where \"pre\" is defined\n",
    "# michael: every time you do something with an activation. might not mean anything bc \"residual stream\" is not an action (unlike, say, adding the attention to the residual)\n",
    "# https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/full-merm.svg\n",
    "# model.add_hook(\"blocks.0.hook_resid_pre\", replace_with_sae_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay...nice...looks like most of the output logits have changed now $$that this single layer is using SAE activations instead of the residual stream!\n",
    "\n",
    "Next step: checking the correlation of 55, 4\n",
    "\n",
    "how do I do that...\n",
    "okay well how did we compute correlations initially?\n",
    "\n",
    "First I need to collect the sae_activations but this time with the SAE being used to influence the second layer's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LayerFeatures:\n",
    "    layer_idx: int\n",
    "    feature_idxes: List[int]\n",
    "\n",
    "@dataclass\n",
    "class AggregatorConfig:\n",
    "    layer_1: LayerFeatures\n",
    "    layer_2: LayerFeatures\n",
    "    \n",
    "class BatchedPearson:\n",
    "    def __init__(self, agg_conf: AggregatorConfig):\n",
    "        \"\"\"Calculates the pair-wise Pearson correlation of two tensors that are provided batch-wise.\n",
    "        \"\"\"\n",
    "        self.agg_conf = agg_conf\n",
    "        shape = (len(agg_conf.layer_1.feature_idxes), len(agg_conf.layer_2.feature_idxes))\n",
    "        self.count = 0\n",
    "\n",
    "        self.sums_1 = torch.zeros(shape[0])\n",
    "        self.sums_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_of_squares_1 = torch.zeros(shape[0])\n",
    "        self.sums_of_squares_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_1_2 = torch.zeros(shape)\n",
    "\n",
    "        self.nonzero_counts_1 = torch.zeros(shape[0])\n",
    "        self.nonzero_counts_2 = torch.zeros(shape[1])\n",
    "\n",
    "    def process(self, tensor_1, tensor_2):\n",
    "        self.count += tensor_1.shape[-1]\n",
    "\n",
    "        self.sums_1 += tensor_1.sum(dim=-1)\n",
    "        self.sums_2 += tensor_2.sum(dim=-1)\n",
    "\n",
    "        self.sums_of_squares_1 += (tensor_1 ** 2).sum(dim=-1)\n",
    "        self.sums_of_squares_2 += (tensor_2 ** 2).sum(dim=-1)\n",
    "\n",
    "        self.sums_1_2 += einops.einsum(tensor_1, tensor_2, 'f1 t, f2 t -> f1 f2')\n",
    "\n",
    "        self.nonzero_counts_1 += tensor_1.count_nonzero(dim=-1)\n",
    "        self.nonzero_counts_2 += tensor_2.count_nonzero(dim=-1)\n",
    "\n",
    "    def finalize(self):\n",
    "        means_1 = self.sums_1 / self.count\n",
    "        means_2 = self.sums_2 / self.count\n",
    "\n",
    "        # Compute the covariance and variances\n",
    "        covariances = (self.sums_1_2 / self.count) - einops.einsum(means_1, means_2, 'f1, f2 -> f1 f2')\n",
    "\n",
    "        variances_1 = (self.sums_of_squares_1 / self.count) - (means_1 ** 2)\n",
    "        variances_2 = (self.sums_of_squares_2 / self.count) - (means_2 ** 2)\n",
    "\n",
    "        stds_1 = torch.sqrt(variances_1).unsqueeze(1)\n",
    "        stds_2 = torch.sqrt(variances_2).unsqueeze(0)\n",
    "\n",
    "        # Compute the Pearson correlation coefficient\n",
    "        correlations = covariances / stds_1 / stds_2\n",
    "\n",
    "        return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:25<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "agg_confs = [\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(0, [10715]),\n",
    "        LayerFeatures(1, [20175]),\n",
    "    ),\n",
    "]\n",
    "aggs = [BatchedPearson(agg_conf) for agg_conf in agg_confs]\n",
    "sae_activations = torch.empty(model.cfg.n_layers, d_sae, batch_size * context_size)\n",
    "\n",
    "def replace_acts_with_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # replaces the residual stream activations with SAE activations\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    return sae(activations)\n",
    "\n",
    "def emit_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # emits what the SAE activations would be for these input activations\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_acts = sae.encode(activations)\n",
    "    sae_activations[hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "    return activations\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        # okay so if we have a hook which is doing SAE embeddings and returning them then...we need to be...\n",
    "        # i think it'll be easier if BOTH functions save SAEs, but only one ofthem returns the SAEs and other returns original acts\n",
    "        model.reset_hooks()\n",
    "        # TODO(optimization): collapse these into a single function which decides whether to \n",
    "        # emit the SAE activations or return the originals\n",
    "        model.add_hook(\n",
    "            lambda name: name.endswith('.hook_resid_pre'),\n",
    "            emit_sae,\n",
    "        )\n",
    "        model.run_with_hooks(batch_tokens)\n",
    "        for agg in aggs:\n",
    "            agg.process(\n",
    "                sae_activations[agg.agg_conf.layer_1.layer_idx, agg.agg_conf.layer_1.feature_idxes],\n",
    "                sae_activations[agg.agg_conf.layer_2.layer_idx, agg.agg_conf.layer_2.feature_idxes]\n",
    "            )\n",
    "    pearson_correlations = [aggregator.finalize() for aggregator in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_correlations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's what I suspected\n",
    "Now I'm going to recompute it but I'll see what happens when I feed the lossy first layer into the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:33<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "agg_confs = [\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(0, [10715]),\n",
    "        LayerFeatures(1, [20175]),\n",
    "    ),\n",
    "]\n",
    "aggs = [BatchedPearson(agg_conf) for agg_conf in agg_confs]\n",
    "sae_activations = torch.empty(model.cfg.n_layers, d_sae, batch_size * context_size)\n",
    "\n",
    "def replace_acts_with_lossy_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # replaces the residual stream activations with SAE activations\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    return sae(activations)\n",
    "\n",
    "def emit_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # emits what the SAE activations would be for these input activations\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_acts = sae.encode(activations)\n",
    "    sae_activations[hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "    return activations\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        # okay so if we have a hook which is doing SAE embeddings and returning them then...we need to be...\n",
    "        # i think it'll be easier if BOTH functions save SAEs, but only one ofthem returns the SAEs and other returns original acts\n",
    "        model.reset_hooks()\n",
    "        # TODO(optimization): collapse these into a single function which decides whether to \n",
    "        # emit the SAE activations or return the originals\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    lambda name: name.endswith('.hook_resid_pre'), # we emit the SAE activations no matter what\n",
    "                    emit_sae,\n",
    "                ),\n",
    "                (\n",
    "                    \"blocks.0.hook_resid_pre\", # TODO: is this right?? is this just a token + positional embedding??\n",
    "                    replace_acts_with_lossy_sae,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        for agg in aggs:\n",
    "            agg.process(\n",
    "                sae_activations[agg.agg_conf.layer_1.layer_idx, agg.agg_conf.layer_1.feature_idxes],\n",
    "                sae_activations[agg.agg_conf.layer_2.layer_idx, agg.agg_conf.layer_2.feature_idxes]\n",
    "            )\n",
    "    pearson_correlations = [aggregator.finalize() for aggregator in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9909]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_correlations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm...they're even *more* correlated.\n",
    "Is that what I would expect? Maybe...if now, the layer1 feature value is lower on avg, so it would be more \"in line\" with layer 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work on multiple feature pairs within a single layer pair\n",
    "All I'd need to do is extend AggConf to operate on multiple pairs of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work across multiple layer pairs\n",
    "\n",
    "the naive way to do this would be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_sae_from_lossy_reconstruction(activations, hook, layer_idx_being_reconstructed):\n",
    "    sae_activations[layer_idx_being_reconstructed][hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "\n",
    "sae_activations = t.empty(\n",
    "    model.cfg.n_layers, # represents which layer is being reconstructed in this pass\n",
    "    model.cfg.n_layers, # represents which layer's activations are being collected in these values\n",
    "    d_sae, # the index of each activation\n",
    "    batch_size * context_size # the tokens which are contributing to the activations\n",
    ")\n",
    "for layer_idx_to_lossily_reconstruct in range(len(layers)):\n",
    "    model.run_with_hooks(\n",
    "        (\n",
    "            lambda name: name.endswith('.hook_resid_pre'),\n",
    "            partial(emit_sae, layer_idx_to_lossily_reconstruct),\n",
    "        ),\n",
    "        (\n",
    "            f\"blocks.{layer_idx_to_lossily_reconstruct}.hook_resid_pre\",\n",
    "            replace_acts_with_lossy_sae,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring ablation impact on correlations\n",
    "\n",
    "Okay, so now we're able to reconstruct residual streams out of SAEs without having done anything to do the SAEs. But what we really want to do is to ablate one of the SAE features and see what that does to the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# unablated_f2_acts = t.empty(num_tokens)\n",
    "# ablated_f2_acts = t.empty(num_tokens)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "L0F1-> L1F2\n",
    "1. what is F2's value when F1 is not ablated\n",
    "2. what is F2's value when F1 is ablated\n",
    "x = input/tokens\n",
    "m(x) = f2(x) - f2(x|a)\n",
    "m2 = mse(m(x) for x in all_inputs) \n",
    "\n",
    "\n",
    "1. first layer, we save the error terms\n",
    "2. in the second layer, we store the desired feature's activations\n",
    "\n",
    "in the second pass\n",
    "1. in the first layer, we ablate, reconstruct, and add the error term, return\n",
    "2. in the second layer, we store feature activations after receiving ablated input\n",
    "\"\"\"\n",
    "sae_errors = t.empty(batch_size, context_size, model.cfg.d_model)\n",
    "feature_2_unablated_acts = t.empty(batch_size, context_size)\n",
    "feature_2_ablated_acts = t.empty(batch_size, context_size)\n",
    "\n",
    "# TODO: rename\n",
    "def save_error_terms(activations: t.Tensor, hook: HookPoint):\n",
    "    global sae_errors\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    reconstructed_acts = sae(activations)\n",
    "    sae_errors = activations - reconstructed_acts \n",
    "    return activations\n",
    "    \n",
    "def save_feature_2_acts(activations: t.Tensor, hook: HookPoint, feature_idx: int, ablated: bool):\n",
    "    global feature_2_ablated_acts\n",
    "    global feature_2_unablated_acts\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_feats = sae.encode(activations)\n",
    "    if ablated:\n",
    "        feature_2_ablated_acts = sae_feats[:,:,feature_idx]\n",
    "    else:\n",
    "        feature_2_unablated_acts = sae_feats[:,:,feature_idx]\n",
    "    return activations\n",
    "\n",
    "def ablate_and_reconstruct_with_errors(activations: t.Tensor, hook: HookPoint, feature_idx: int):\n",
    "    global sae_errors\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_feats = sae.encode(activations)\n",
    "    sae_feats[:,:,feature_idx] = 0    \n",
    "    return sae.decode(sae_feats) + sae_errors # we assume that the hooks are called in the correct order s.t. these errors correspond to the same tokens\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        model.reset_hooks()\n",
    "        # collect the unablated activations\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    \"blocks.0.hook_resid_pre\", # TODO: parameterize\n",
    "                    save_error_terms,\n",
    "                ),\n",
    "                (\n",
    "                    \"blocks.1.hook_resid_pre\", # TODO: parameterize\n",
    "                    partial(save_feature_2_acts, feature_idx=20175, ablated=False) # TODO :parameterize\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        # collect the activations after ablation\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    \"blocks.0.hook_resid_pre\", # TODO: parameterize\n",
    "                    partial(ablate_and_reconstruct_with_errors, feature_idx=10715)\n",
    "                ),\n",
    "                (\n",
    "                    \"blocks.1.hook_resid_pre\", # TODO: parameterize\n",
    "                    partial(save_feature_2_acts, feature_idx=20175, ablated=True) # TODO :parameterize\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        break\n",
    "# # adjacent_feat_pairs \n",
    "# def compute_ablation_impact(pairs: [(layer_0, feat_51, feat_853)], tokens):\n",
    "#     \"\"\"\n",
    "#     1. take layer 1 resid\n",
    "#         a. compute reconstruction error\n",
    "#             error = resid - sae(resid)\n",
    "#         b. ablate a SAE feature\n",
    "#             sae_acts = sae.encode(resid)\n",
    "#             sae_acts[sae_feat_idx] = 0\n",
    "#     2. return sae.decode(sae_acts) + error\n",
    "#     \"\"\"\n",
    "#     return [\n",
    "#         [(layer_0, feat_51, feat_853, )]\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (feature_2_ablated_acts - feature_2_unablated_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='mps:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_2_unablated_acts.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='mps:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_2_ablated_acts.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='mps:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=4000)\n",
    "diff.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT STEPS:\n",
    "1. figure out how to accumulate over batches\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say that we've come up with a bunch of (first_layer_idx, first_layer_feature_idx, next_layer_feature_idx, correlation) tuples e.g.\n",
    "correlations = [\n",
    "    (0, 10305, 403, 0.95), # feature 10305 in layer 0 and feature 403 in layer 1 have a 0.95 correlation\n",
    "]\n",
    "# and now we want to see \"what would happen if we ablated the first features to 0? how would that affect each second feature? would they still be highly correlated?\"\n",
    "# so we'd want to end up with something like\n",
    "ablated_correlations = [\n",
    "    (0, 10305, 403, 0.90) # hm.actually...what WOULD we expect this to be?\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: what do we expect the new correlations to be between previously-correlated feature pairs after we ablate the first feature in the pair? if it used to look like\n",
    "```\n",
    "feature_1_feature_2_acts = [\n",
    "    (4, 4.2),\n",
    "    (3.1, 2.9),\n",
    "    (5.5, 5.7),\n",
    "    (0.5, 0.8)\n",
    "]\n",
    "```\n",
    "and now it's like\n",
    "```\n",
    "feature_1_feature_2_acts = [\n",
    "    (0, 0.5)\n",
    "    (0, 0.8),\n",
    "    (0, 0.5),\n",
    "    (0, 0.5)\n",
    "]\n",
    "```\n",
    "\n",
    "then that indicates that feature 1 DOES cause feature 2, right? but wouldn't these have a correlation of nan? so what am I supposed to measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spar2024.slack.com/archives/C078944NFD4/p1720459095680359"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
