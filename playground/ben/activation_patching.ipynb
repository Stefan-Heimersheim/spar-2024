{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clamping SAE features in one layer and measuring effects on the subsequent layer\n",
    "## Plan described [here](https://spar2024.slack.com/archives/C0794GNT8KS/p1719950740186749?thread_ts=1719934219.491869&cid=C0794GNT8KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small set of correlations to play around with\n",
    "pearson_0_1_small: 'f' = t.load('../../experiments/pearson_0_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest correlations\n",
    "def create_value_tensor(matrix: 'f,f') -> 'f*f,3':\n",
    "    m, _ = matrix.shape\n",
    "    \n",
    "    # Step 1: Flatten the matrix (shape: [m*m])\n",
    "    flattened_matrix = matrix.flatten()\n",
    "    \n",
    "    # Step 2: Create row and column indices\n",
    "    row_indices = t.arange(m).repeat_interleave(m)\n",
    "    col_indices = t.arange(m).repeat(m)\n",
    "    \n",
    "    # Step 3: Create the final tensor with indices and values\n",
    "    values = flattened_matrix\n",
    "    result = t.stack((row_indices, col_indices, values), dim=1)\n",
    "    \n",
    "    # Step 4: Sort the result tensor by values\n",
    "    sorted_result = result[t.argsort(result[:, 2], descending=True)]\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = create_value_tensor(pearson_0_1_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5000e+01, 4.0000e+00, 1.0047e+00],\n",
       "        [1.0000e+01, 5.5000e+01, 1.0045e+00],\n",
       "        [5.3000e+01, 4.5000e+01, 1.0043e+00],\n",
       "        ...,\n",
       "        [1.4000e+01, 8.0000e+00, 2.0628e-03],\n",
       "        [7.4000e+01, 9.6000e+01, 1.7489e-03],\n",
       "        [1.3000e+01, 4.4000e+01, 1.6410e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0047) tensor(0.9270)\n"
     ]
    }
   ],
   "source": [
    "print(pearson_0_1_small[55, 4], pearson_0_1_small[1, 55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, I'll pick l1f55 and l2f4 as my pair. something to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the correlation when we pass the residual stream that's reconstructed from the SAE features - NO clamping\n",
    "As mentioned at the end of June, normally the SAE features values are read by us and discarded - they are not passed back into the model for inference.\n",
    "However, if we are going to be clamping an SAE feature and seeing its impact downstream, then we need to first see what happens when we pass the SAE features downstream with no clamping - because the mere act of projecting a residual stream into an SAE space and then back into residual stream space is a lossy operation (even though the SAE is supposed to represent the residual stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benlerner/work/spar-2024/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae, _, _ = SAE.from_pretrained(release=\"gpt2-small-res-jb\", sae_id=f\"blocks.0.hook_resid_pre\", device=device)\n",
    "# okay...so I need a hook...I need a hook which will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# These hyperparameters are used to pre-process the data\n",
    "context_size = sae.cfg.context_size\n",
    "prepend_bos = sae.cfg.prepend_bos\n",
    "batch_size = 32\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=context_size,\n",
    "    add_bos_token=prepend_bos,\n",
    ")\n",
    "\n",
    "tokens = token_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Reduce dataset for faster experimentation\n",
    "tokens = tokens[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tokens, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with t.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        _, model_cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "        # TODO: what's going on here?\n",
    "        feat_acts = sae.encode[model_cache.cfg.hook_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAMPING an SAE feature to 0\n",
    "looking through https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.patching.html#transformer_lens.patching.generic_activation_patch and Iâ€™m not seeing much in the way of ablations...patching seems based on seeing how activations change by varying inputs, which isn't what I'm going to be doing.\n",
    "\n",
    "Well...what am I doing, exactly? I guess I'm going to be...clamping the feature to `0` for all residual stream positions? for a single residual stream position? TODO: figure this out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
