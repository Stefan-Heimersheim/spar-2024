{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm import tqdm, trange\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clamping SAE features in one layer and measuring effects on the subsequent layer\n",
    "## Plan described [here](https://spar2024.slack.com/archives/C0794GNT8KS/p1719950740186749?thread_ts=1719934219.491869&cid=C0794GNT8KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small set of correlations to play around with\n",
    "pearson_0_1_small: 'f' = t.load('../../data/res_jb_sae_feature_correlation_pearson_0_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest correlations\n",
    "def create_value_tensor(matrix: 'f,f') -> 'f*f,3':\n",
    "    m, _ = matrix.shape\n",
    "    \n",
    "    # Step 1: Flatten the matrix (shape: [m*m])\n",
    "    flattened_matrix = matrix.flatten()\n",
    "    \n",
    "    # Step 2: Create row and column indices\n",
    "    row_indices = t.arange(m).repeat_interleave(m)\n",
    "    col_indices = t.arange(m).repeat(m)\n",
    "    \n",
    "    # Step 3: Create the final tensor with indices and values\n",
    "    values = flattened_matrix\n",
    "    result = t.stack((row_indices, col_indices, values), dim=1)\n",
    "    \n",
    "    # Step 4: Sort the result tensor by values\n",
    "    sorted_result = result[t.argsort(result[:, 2], descending=True)]\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9526e-05, -4.4647e-05, -9.5376e-05,  ..., -2.0311e-05,\n",
       "         -1.4521e-05, -5.3107e-05],\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-1.6616e-04, -2.5125e-04, -5.3673e-04,  ..., -1.1430e-04,\n",
       "         -8.1718e-05,  3.1849e-04],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-1.5595e-04, -2.3582e-04, -5.0376e-04,  ..., -1.0728e-04,\n",
       "         -7.6699e-05, -2.8050e-04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_0_1_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = create_value_tensor(pearson_0_1_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to identify rows where the last column is not NaN\n",
    "not_nan_mask = ~torch.isnan(ranked_features[:, 2])\n",
    "\n",
    "# Create a mask to identify rows where the last column is less than 1\n",
    "less_than_one_mask = ranked_features[:, 2] < 0.99\n",
    "\n",
    "# Combine masks using logical AND\n",
    "combined_mask = not_nan_mask & less_than_one_mask\n",
    "\n",
    "# Use the combined mask to filter the rows\n",
    "filtered_tensor = ranked_features[combined_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0715e+04,  2.0175e+04,  9.8999e-01],\n",
       "        [ 1.8389e+04,  5.4670e+03,  9.8998e-01],\n",
       "        [ 6.2600e+03,  1.3641e+04,  9.8998e-01],\n",
       "        ...,\n",
       "        [ 1.3725e+04,  9.4870e+03, -1.4345e-01],\n",
       "        [ 1.3725e+04,  1.5370e+04, -1.4875e-01],\n",
       "        [ 1.3725e+04,  2.2119e+04, -1.5495e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9900)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_0_1_small[10715, 20175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the correlation when we pass the residual stream that's reconstructed from the SAE features - NO clamping\n",
    "As mentioned at the end of June, normally the SAE features values are read by us and discarded - they are not passed back into the model for inference.\n",
    "However, if we are going to be clamping an SAE feature and seeing its impact downstream, then we need to first see what happens when we pass the SAE features downstream with no clamping - because the mere act of projecting a residual stream into an SAE space and then back into residual stream space is a lossy operation (even though the SAE is supposed to represent the residual stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benlerner/work/spar-2024/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 12/12 [00:08<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae_id_to_sae = {}\n",
    "for layer in tqdm(list(range(model.cfg.n_layers))):\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=sae_id,\n",
    "        device=device\n",
    "    )\n",
    "    sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "    sae_id_to_sae[sae_id] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# These hyperparameters are used to pre-process the data\n",
    "pre_0_sae_id = \"blocks.0.hook_resid_pre\"\n",
    "pre_0_sae = sae_id_to_sae[pre_0_sae_id]\n",
    "context_size = pre_0_sae.cfg.context_size\n",
    "prepend_bos = pre_0_sae.cfg.prepend_bos\n",
    "d_sae = pre_0_sae.cfg.d_sae\n",
    "batch_size = 32\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=context_size,\n",
    "    add_bos_token=prepend_bos,\n",
    ")\n",
    "\n",
    "tokens = token_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Reduce dataset for faster experimentation\n",
    "tokens = tokens[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tokens, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at compute-pearson-0.py to figure out how hooks work\n",
    "\n",
    "\n",
    "# okay so add_hook takes a function...and then...\n",
    "# https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.hook_points.html#transformer_lens.hook_points.HookPoint.add_hook\n",
    "\n",
    "# TODO: make a lambda function\n",
    "# model.reset_hooks()\n",
    "\n",
    "# TODO: find out where \"pre\" is defined\n",
    "# michael: every time you do something with an activation. might not mean anything bc \"residual stream\" is not an action (unlike, say, adding the attention to the residual)\n",
    "# https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/full-merm.svg\n",
    "# model.add_hook(\"blocks.0.hook_resid_pre\", replace_with_sae_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay...nice...looks like most of the output logits have changed now $$that this single layer is using SAE activations instead of the residual stream!\n",
    "\n",
    "Next step: checking the correlation of 55, 4\n",
    "\n",
    "how do I do that...\n",
    "okay well how did we compute correlations initially?\n",
    "\n",
    "First I need to collect the sae_activations but this time with the SAE being used to influence the second layer's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LayerFeatures:\n",
    "    layer_idx: int\n",
    "    feature_idxes: List[int]\n",
    "\n",
    "@dataclass\n",
    "class AggregatorConfig:\n",
    "    layer_1: LayerFeatures\n",
    "    layer_2: LayerFeatures\n",
    "    \n",
    "class BatchedPearson:\n",
    "    def __init__(self, agg_conf: AggregatorConfig):\n",
    "        \"\"\"Calculates the pair-wise Pearson correlation of two tensors that are provided batch-wise.\n",
    "        \"\"\"\n",
    "        self.agg_conf = agg_conf\n",
    "        shape = (len(agg_conf.layer_1.feature_idxes), len(agg_conf.layer_2.feature_idxes))\n",
    "        self.count = 0\n",
    "\n",
    "        self.sums_1 = torch.zeros(shape[0])\n",
    "        self.sums_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_of_squares_1 = torch.zeros(shape[0])\n",
    "        self.sums_of_squares_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_1_2 = torch.zeros(shape)\n",
    "\n",
    "        self.nonzero_counts_1 = torch.zeros(shape[0])\n",
    "        self.nonzero_counts_2 = torch.zeros(shape[1])\n",
    "\n",
    "    def process(self, tensor_1, tensor_2):\n",
    "        self.count += tensor_1.shape[-1]\n",
    "\n",
    "        self.sums_1 += tensor_1.sum(dim=-1)\n",
    "        self.sums_2 += tensor_2.sum(dim=-1)\n",
    "\n",
    "        self.sums_of_squares_1 += (tensor_1 ** 2).sum(dim=-1)\n",
    "        self.sums_of_squares_2 += (tensor_2 ** 2).sum(dim=-1)\n",
    "\n",
    "        self.sums_1_2 += einops.einsum(tensor_1, tensor_2, 'f1 t, f2 t -> f1 f2')\n",
    "\n",
    "        self.nonzero_counts_1 += tensor_1.count_nonzero(dim=-1)\n",
    "        self.nonzero_counts_2 += tensor_2.count_nonzero(dim=-1)\n",
    "\n",
    "    def finalize(self):\n",
    "        means_1 = self.sums_1 / self.count\n",
    "        means_2 = self.sums_2 / self.count\n",
    "\n",
    "        # Compute the covariance and variances\n",
    "        covariances = (self.sums_1_2 / self.count) - einops.einsum(means_1, means_2, 'f1, f2 -> f1 f2')\n",
    "\n",
    "        variances_1 = (self.sums_of_squares_1 / self.count) - (means_1 ** 2)\n",
    "        variances_2 = (self.sums_of_squares_2 / self.count) - (means_2 ** 2)\n",
    "\n",
    "        stds_1 = torch.sqrt(variances_1).unsqueeze(1)\n",
    "        stds_2 = torch.sqrt(variances_2).unsqueeze(0)\n",
    "\n",
    "        # Compute the Pearson correlation coefficient\n",
    "        correlations = covariances / stds_1 / stds_2\n",
    "\n",
    "        return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:25<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "agg_confs = [\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(0, [10715]),\n",
    "        LayerFeatures(1, [20175]),\n",
    "    ),\n",
    "]\n",
    "aggs = [BatchedPearson(agg_conf) for agg_conf in agg_confs]\n",
    "sae_activations = torch.empty(model.cfg.n_layers, d_sae, batch_size * context_size)\n",
    "\n",
    "def replace_acts_with_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # replaces the residual stream activations with SAE activations\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    return sae(activations)\n",
    "\n",
    "def emit_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # emits what the SAE activations would be for these input activations\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_acts = sae.encode(activations)\n",
    "    sae_activations[hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "    return activations\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        # okay so if we have a hook which is doing SAE embeddings and returning them then...we need to be...\n",
    "        # i think it'll be easier if BOTH functions save SAEs, but only one ofthem returns the SAEs and other returns original acts\n",
    "        model.reset_hooks()\n",
    "        # TODO(optimization): collapse these into a single function which decides whether to \n",
    "        # emit the SAE activations or return the originals\n",
    "        model.add_hook(\n",
    "            lambda name: name.endswith('.hook_resid_pre'),\n",
    "            emit_sae,\n",
    "        )\n",
    "        model.run_with_hooks(batch_tokens)\n",
    "        for agg in aggs:\n",
    "            agg.process(\n",
    "                sae_activations[agg.agg_conf.layer_1.layer_idx, agg.agg_conf.layer_1.feature_idxes],\n",
    "                sae_activations[agg.agg_conf.layer_2.layer_idx, agg.agg_conf.layer_2.feature_idxes]\n",
    "            )\n",
    "    pearson_correlations = [aggregator.finalize() for aggregator in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlations[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_feats = t.concat([\n",
    "    create_value_tensor(pc)\n",
    "    for pc in pearson_correlations\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "pearson_correlations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's what I suspected\n",
    "Now I'm going to recompute it but I'll see what happens when I feed the lossy first layer into the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:46<00:00,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "agg_confs = [\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(0, [10715]),\n",
    "        LayerFeatures(1, [20175]),\n",
    "    ),\n",
    "]\n",
    "aggs = [BatchedPearson(agg_conf) for agg_conf in agg_confs]\n",
    "sae_activations = torch.empty(model.cfg.n_layers, d_sae, batch_size * context_size)\n",
    "\n",
    "def replace_acts_with_lossy_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # replaces the residual stream activations with SAE activations\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    return sae(activations)\n",
    "\n",
    "def emit_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # emits what the SAE activations would be for these input activations\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_acts = sae.encode(activations)\n",
    "    sae_activations[hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "    return activations\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        # okay so if we have a hook which is doing SAE embeddings and returning them then...we need to be...\n",
    "        # i think it'll be easier if BOTH functions save SAEs, but only one ofthem returns the SAEs and other returns original acts\n",
    "        model.reset_hooks()\n",
    "        # TODO(optimization): collapse these into a single function which decides whether to \n",
    "        # emit the SAE activations or return the originals\n",
    "        model.add_hook(\n",
    "            lambda name: name.endswith('.hook_resid_pre'), # we emit the SAE activations no matter what\n",
    "            emit_sae,\n",
    "        )\n",
    "        model.add_hook(\n",
    "            \"blocks.0.hook_resid_pre\", # TODO: is this right?? is this just a token + positional embedding??\n",
    "            replace_acts_with_lossy_sae,\n",
    "        )\n",
    "        model.run_with_hooks(batch_tokens)\n",
    "        for agg in aggs:\n",
    "            agg.process(\n",
    "                sae_activations[agg.agg_conf.layer_1.layer_idx, agg.agg_conf.layer_1.feature_idxes],\n",
    "                sae_activations[agg.agg_conf.layer_2.layer_idx, agg.agg_conf.layer_2.feature_idxes]\n",
    "            )\n",
    "    pearson_correlations = [aggregator.finalize() for aggregator in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9909]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_correlations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm...they're even *more* correlated.\n",
    "Is that what I would expect? Maybe...if now, the layer1 feature value is lower on avg, so it would be more \"in line\" with layer 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work on multiple feature pairs within a single layer pair\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work across multiple layer pairs\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
