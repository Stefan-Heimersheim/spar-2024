{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clamping SAE features in one layer and measuring effects on the subsequent layer\n",
    "## Plan described [here](https://spar2024.slack.com/archives/C0794GNT8KS/p1719950740186749?thread_ts=1719934219.491869&cid=C0794GNT8KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small set of correlations to play around with\n",
    "pearson_0_1_small: 'f' = t.load('../../data/pearson_0_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest correlations\n",
    "def create_value_tensor(matrix: 'f,f') -> 'f*f,3':\n",
    "    m, _ = matrix.shape\n",
    "    \n",
    "    # Step 1: Flatten the matrix (shape: [m*m])\n",
    "    flattened_matrix = matrix.flatten()\n",
    "    \n",
    "    # Step 2: Create row and column indices\n",
    "    row_indices = t.arange(m).repeat_interleave(m)\n",
    "    col_indices = t.arange(m).repeat(m)\n",
    "    \n",
    "    # Step 3: Create the final tensor with indices and values\n",
    "    values = flattened_matrix\n",
    "    result = t.stack((row_indices, col_indices, values), dim=1)\n",
    "    \n",
    "    # Step 4: Sort the result tensor by values\n",
    "    sorted_result = result[t.argsort(result[:, 2], descending=True)]\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = create_value_tensor(pearson_0_1_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0047) tensor(0.9270)\n"
     ]
    }
   ],
   "source": [
    "print(pearson_0_1_small[55, 4], pearson_0_1_small[1, 55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the correlation when we pass the residual stream that's reconstructed from the SAE features - NO clamping\n",
    "As mentioned at the end of June, normally the SAE features values are read by us and discarded - they are not passed back into the model for inference.\n",
    "However, if we are going to be clamping an SAE feature and seeing its impact downstream, then we need to first see what happens when we pass the SAE features downstream with no clamping - because the mere act of projecting a residual stream into an SAE space and then back into residual stream space is a lossy operation (even though the SAE is supposed to represent the residual stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benlerner/work/spar-2024/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 12/12 [00:06<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae_id_to_sae = {}\n",
    "for layer in tqdm(list(range(model.cfg.n_layers))):\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=sae_id,\n",
    "        device=device\n",
    "    )\n",
    "    sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "    sae_id_to_sae[sae_id] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# These hyperparameters are used to pre-process the data\n",
    "random_sae_id = \"blocks.0.hook_resid_pre\"\n",
    "random_sae = sae_id_to_sae[random_sae_id]\n",
    "context_size = random_sae.cfg.context_size\n",
    "prepend_bos = random_sae.cfg.prepend_bos\n",
    "batch_size = 32\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=context_size,\n",
    "    add_bos_token=prepend_bos,\n",
    ")\n",
    "\n",
    "tokens = token_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Reduce dataset for faster experimentation\n",
    "tokens = tokens[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tokens, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at compute-pearson-0.py to figure out how hooks work\n",
    "def replace_with_sae_output(activations, hook):\n",
    "    \"\"\"so I guess I'm going to\n",
    "    1) take the activations out of some specific layer (which i can generalize later)\n",
    "    2) pass them through an SAE, get back the same shape (but it'll look a little different bc it's lost some data)\n",
    "    3) return that (??)\n",
    "    \n",
    "    well we're not returning anything here...wtf is a hook?? it's a function that runs while a model is executing..ok...\n",
    "    so do I \n",
    "    1. modify the layer directly\n",
    "    2. return what i want?\n",
    "    \n",
    "    okay let's say that i need to do in-memory modification. what would that look like?\n",
    "    I have this \"hook\". it's a function..what type is it? idk. well I guess I'm in the hook right now.\n",
    "    wait..no..this is confusing. this function is called \"hook\". and yet...it ITSELF contains a hook. HUH? what's the hook??\n",
    "    \n",
    "    okay a \"hookpoint\" is... https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/hook_points.py#L64 \n",
    "    a wrapper around a nn.Module\n",
    "    \n",
    "    and then let's see what add_hook does\n",
    "    all of this seems to be wrapping pytorch.nn.Module's register_forward_hook https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook\n",
    "    \n",
    "    If with_kwargs is False or not specified, the input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called. The hook should have the following signature:\n",
    "    hook(module, args, output) -> None or modified output\n",
    "\n",
    "    okay so I need to create a hook in which...it's going to modify the output.\n",
    "    but then...what's going on with this `hook`\n",
    "    \"\"\"\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    new_activations = sae(activations)\n",
    "    # test_new_acts = sae.decode(sae.encode(activations))\n",
    "    # print(f\"making sure I know how these work: {t.sum(new_activations != test_new_acts).item()}\")\n",
    "    # print(new_activations - activations)\n",
    "    # print(f\"avg diff activations: {t.sum(new_activations != activations).item()}\")\n",
    "    return new_activations \n",
    "\n",
    "# okay so add_hook takes a function...and then...\n",
    "# https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.hook_points.html#transformer_lens.hook_points.HookPoint.add_hook\n",
    "\n",
    "# TODO: make a lambda function\n",
    "# model.reset_hooks()\n",
    "\n",
    "# TODO: find out where \"pre\" is defined\n",
    "# michael: every time you do something with an activation. might not mean anything bc \"residual stream\" is not an action (unlike, say, adding the attention to the residual)\n",
    "# model.add_hook(\"blocks.0.hook_resid_pre\", replace_with_sae_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        orig = model(batch_tokens)\n",
    "        reconstructed = model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    random_sae_id,\n",
    "                    replace_with_sae_output,\n",
    "                )\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(orig == reconstructed).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay...nice...looks like most of the output logits have changed now that this single layer is using SAE activations instead of the residual stream!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAMPING an SAE feature to 0\n",
    "looking through https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.patching.html#transformer_lens.patching.generic_activation_patch and I’m not seeing much in the way of ablations...patching seems based on seeing how activations change by varying inputs, which isn't what I'm going to be doing.\n",
    "\n",
    "Well...what am I doing, exactly? I guess I'm going to be...clamping the feature to `0` for all residual stream positions? for a single residual stream position? TODO: figure this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
