{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm import tqdm, trange\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clamping SAE features in one layer and measuring effects on the subsequent layer\n",
    "## Plan described [here](https://spar2024.slack.com/archives/C0794GNT8KS/p1719950740186749?thread_ts=1719934219.491869&cid=C0794GNT8KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small set of correlations to play around with\n",
    "pearson_0_1_small: 'f' = t.load('../../data/pearson_0_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest correlations\n",
    "def create_value_tensor(matrix: 'f,f') -> 'f*f,3':\n",
    "    m, _ = matrix.shape\n",
    "    \n",
    "    # Step 1: Flatten the matrix (shape: [m*m])\n",
    "    flattened_matrix = matrix.flatten()\n",
    "    \n",
    "    # Step 2: Create row and column indices\n",
    "    row_indices = t.arange(m).repeat_interleave(m)\n",
    "    col_indices = t.arange(m).repeat(m)\n",
    "    \n",
    "    # Step 3: Create the final tensor with indices and values\n",
    "    values = flattened_matrix\n",
    "    result = t.stack((row_indices, col_indices, values), dim=1)\n",
    "    \n",
    "    # Step 4: Sort the result tensor by values\n",
    "    sorted_result = result[t.argsort(result[:, 2], descending=True)]\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = create_value_tensor(pearson_0_1_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0047) tensor(0.9270)\n"
     ]
    }
   ],
   "source": [
    "print(pearson_0_1_small[55, 4], pearson_0_1_small[1, 55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the correlation when we pass the residual stream that's reconstructed from the SAE features - NO clamping\n",
    "As mentioned at the end of June, normally the SAE features values are read by us and discarded - they are not passed back into the model for inference.\n",
    "However, if we are going to be clamping an SAE feature and seeing its impact downstream, then we need to first see what happens when we pass the SAE features downstream with no clamping - because the mere act of projecting a residual stream into an SAE space and then back into residual stream space is a lossy operation (even though the SAE is supposed to represent the residual stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benlerner/work/spar-2024/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:04<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae_id_to_sae = {}\n",
    "for layer in tqdm(list(range(model.cfg.n_layers))):\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=sae_id,\n",
    "        device=device\n",
    "    )\n",
    "    sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "    sae_id_to_sae[sae_id] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# These hyperparameters are used to pre-process the data\n",
    "pre_0_sae_id = \"blocks.0.hook_resid_pre\"\n",
    "pre_0_sae = sae_id_to_sae[pre_0_sae_id]\n",
    "context_size = pre_0_sae.cfg.context_size\n",
    "prepend_bos = pre_0_sae.cfg.prepend_bos\n",
    "d_sae = pre_0_sae.cfg.d_sae\n",
    "batch_size = 32\n",
    "\n",
    "dataset = load_dataset(path=\"NeelNanda/pile-10k\", split=\"train\", streaming=False)\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=context_size,\n",
    "    add_bos_token=prepend_bos,\n",
    ")\n",
    "\n",
    "tokens = token_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Reduce dataset for faster experimentation\n",
    "tokens = tokens[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tokens, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at compute-pearson-0.py to figure out how hooks work\n",
    "\n",
    "\n",
    "# okay so add_hook takes a function...and then...\n",
    "# https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.hook_points.html#transformer_lens.hook_points.HookPoint.add_hook\n",
    "\n",
    "# TODO: make a lambda function\n",
    "# model.reset_hooks()\n",
    "\n",
    "# TODO: find out where \"pre\" is defined\n",
    "# michael: every time you do something with an activation. might not mean anything bc \"residual stream\" is not an action (unlike, say, adding the attention to the residual)\n",
    "# https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/full-merm.svg\n",
    "# model.add_hook(\"blocks.0.hook_resid_pre\", replace_with_sae_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay...nice...looks like most of the output logits have changed now $$that this single layer is using SAE activations instead of the residual stream!\n",
    "\n",
    "Next step: checking the correlation of 55, 4\n",
    "\n",
    "how do I do that...\n",
    "okay well how did we compute correlations initially?\n",
    "\n",
    "First I need to collect the sae_activations but this time with the SAE being used to influence the second layer's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LayerFeatures:\n",
    "    layer_idx: int\n",
    "    feature_idxes: List[int]\n",
    "\n",
    "@dataclass\n",
    "class AggregatorConfig:\n",
    "    layer_1: LayerFeatures\n",
    "    layer_2: LayerFeatures\n",
    "    \n",
    "class BatchedPearson:\n",
    "    def __init__(self, agg_conf: AggregatorConfig):\n",
    "        \"\"\"Calculates the pair-wise Pearson correlation of two tensors that are provided batch-wise.\n",
    "        \"\"\"\n",
    "        self.agg_conf = agg_conf\n",
    "        shape = (len(agg_conf.layer_1.feature_idxes), len(agg_conf.layer_2.feature_idxes))\n",
    "        self.count = 0\n",
    "\n",
    "        self.sums_1 = torch.zeros(shape[0])\n",
    "        self.sums_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_of_squares_1 = torch.zeros(shape[0])\n",
    "        self.sums_of_squares_2 = torch.zeros(shape[1])\n",
    "\n",
    "        self.sums_1_2 = torch.zeros(shape)\n",
    "\n",
    "        self.nonzero_counts_1 = torch.zeros(shape[0])\n",
    "        self.nonzero_counts_2 = torch.zeros(shape[1])\n",
    "\n",
    "    def process(self, tensor_1, tensor_2):\n",
    "        self.count += tensor_1.shape[-1]\n",
    "\n",
    "        self.sums_1 += tensor_1.sum(dim=-1)\n",
    "        self.sums_2 += tensor_2.sum(dim=-1)\n",
    "\n",
    "        self.sums_of_squares_1 += (tensor_1 ** 2).sum(dim=-1)\n",
    "        self.sums_of_squares_2 += (tensor_2 ** 2).sum(dim=-1)\n",
    "\n",
    "        self.sums_1_2 += einops.einsum(tensor_1, tensor_2, 'f1 t, f2 t -> f1 f2')\n",
    "\n",
    "        self.nonzero_counts_1 += tensor_1.count_nonzero(dim=-1)\n",
    "        self.nonzero_counts_2 += tensor_2.count_nonzero(dim=-1)\n",
    "\n",
    "    def finalize(self):\n",
    "        means_1 = self.sums_1 / self.count\n",
    "        means_2 = self.sums_2 / self.count\n",
    "\n",
    "        # Compute the covariance and variances\n",
    "        covariances = (self.sums_1_2 / self.count) - einops.einsum(means_1, means_2, 'f1, f2 -> f1 f2')\n",
    "\n",
    "        variances_1 = (self.sums_of_squares_1 / self.count) - (means_1 ** 2)\n",
    "        variances_2 = (self.sums_of_squares_2 / self.count) - (means_2 ** 2)\n",
    "\n",
    "        stds_1 = torch.sqrt(variances_1).unsqueeze(1)\n",
    "        stds_2 = torch.sqrt(variances_2).unsqueeze(0)\n",
    "\n",
    "        # Compute the Pearson correlation coefficient\n",
    "        correlations = covariances / stds_1 / stds_2\n",
    "\n",
    "        return correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:14<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "agg_confs = [\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(8, [0, 1, 2, 3, 4, 5, 6, 7, 8, 14292]),\n",
    "        LayerFeatures(9, [0, 1, 2, 3, 4, 5, 6, 7, 8, 4813]),\n",
    "    ),\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(9, [0, 1, 2, 3, 4, 5, 6, 7, 8, 4813]),\n",
    "        LayerFeatures(10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 23138]),\n",
    "    ),\n",
    "    AggregatorConfig(\n",
    "        LayerFeatures(10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 23138]),\n",
    "        LayerFeatures(11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 701]),\n",
    "    ),\n",
    "]\n",
    "aggs = [BatchedPearson(agg_conf) for agg_conf in agg_confs]\n",
    "sae_activations = torch.empty(model.cfg.n_layers, d_sae, batch_size * context_size)\n",
    "\n",
    "def replace_acts_with_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # replaces the residual stream activations with SAE activations\n",
    "    sae = sae_id_to_sae[hook.name]\n",
    "    return sae(activations)\n",
    "\n",
    "def emit_sae(activations: t.Tensor, hook: HookPoint):\n",
    "    # emits what the SAE activations would be for these input activations\n",
    "    sae: SAE = sae_id_to_sae[hook.name]\n",
    "    sae_acts = sae.encode(activations)\n",
    "    sae_activations[hook.layer()] = einops.rearrange(\n",
    "        sae_acts,\n",
    "        'batch seq features -> features (batch seq)'\n",
    "    )\n",
    "    return activations\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens in tqdm(data_loader):\n",
    "        # okay so if we have a hook which is doing SAE embeddings and returning them then...we need to be...\n",
    "        # i think it'll be easier if BOTH functions save SAEs, but only one ofthem returns the SAEs and other returns original acts\n",
    "        model.reset_hooks()\n",
    "        # TODO(optimization): collapse these into a single function which decides whether to \n",
    "        # emit the SAE activations or return the originals\n",
    "        model.add_hook(\n",
    "            lambda name: name.endswith('.hook_resid_pre'),\n",
    "            emit_sae,\n",
    "        )\n",
    "        model.run_with_hooks(batch_tokens)\n",
    "        for agg in aggs:\n",
    "            agg.process(\n",
    "                sae_activations[agg.agg_conf.layer_1.layer_idx, agg.agg_conf.layer_1.feature_idxes],\n",
    "                sae_activations[agg.agg_conf.layer_2.layer_idx, agg.agg_conf.layer_2.feature_idxes]\n",
    "            )\n",
    "    pearson_correlations = [aggregator.finalize() for aggregator in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ranked_feats = t.concat([\n",
    "    create_value_tensor(pc)\n",
    "    for pc in pearson_correlations\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: figure out how to pick a high correlation pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 3.1148e-03, -1.9471e-04, -6.2550e-04, -1.3654e-03, -4.3121e-04,\n",
       "          -7.5973e-04, -4.6204e-04, -2.0788e-04, -6.0275e-04, -3.9825e-04],\n",
       "         [ 1.2669e-03, -1.7483e-04, -5.6166e-04, -1.2260e-03, -3.8720e-04,\n",
       "          -6.8220e-04, -4.1488e-04, -1.8667e-04, -5.4124e-04, -3.5761e-04],\n",
       "         [-4.0764e-04, -1.0950e-04, -3.5178e-04, -7.5998e-04, -2.4251e-04,\n",
       "           9.7921e-04, -2.5985e-04, -1.1691e-04, -3.3899e-04, -2.2398e-04],\n",
       "         [-3.9907e-04,  5.9122e-05, -3.4439e-04, -7.5175e-04, -2.3741e-04,\n",
       "          -4.1829e-04, -2.5439e-04, -1.1446e-04, -3.3186e-04, -2.1927e-04],\n",
       "         [ 1.0548e-02, -5.3831e-04, -1.7294e-03, -3.7555e-03, -1.1922e-03,\n",
       "          -1.5441e-03, -1.2774e-03, -5.7474e-04, -1.6665e-03, -1.1011e-03],\n",
       "         [-3.5253e-04, -9.4699e-05, -3.0422e-04, -6.6408e-04, -2.0973e-04,\n",
       "          -3.6951e-04, -2.2472e-04, -1.0111e-04, -2.9316e-04, -1.9370e-04],\n",
       "         [-8.6122e-04, -2.3135e-04,  4.5319e-03, -1.6223e-03, -5.1236e-04,\n",
       "          -9.0270e-04, -5.4898e-04,  1.3372e-02, -6.7742e-04, -4.7319e-04],\n",
       "         [-2.8266e-04, -7.5930e-05, -2.4393e-04, -5.3246e-04, -1.6816e-04,\n",
       "           9.1835e-05, -1.8018e-04, -8.1068e-05, -2.3506e-04, -1.5531e-04],\n",
       "         [-4.3671e-04, -4.0950e-04, -9.3975e-04, -3.6149e-03,  1.4305e-03,\n",
       "          -6.4137e-04, -1.2233e-03, -5.5037e-04, -1.3794e-03, -6.2903e-04],\n",
       "         [-4.4885e-04, -1.6540e-04, -5.3136e-04, -1.1532e-03, -3.6631e-04,\n",
       "          -6.4538e-04, -3.9249e-04, -1.7659e-04, -5.1203e-04,  7.5331e-01]]),\n",
       " tensor([[-4.9782e-04, -6.6020e-04, -8.6356e-04, -3.3918e-04, -2.4513e-04,\n",
       "          -2.0674e-04,  7.3718e-04, -9.6903e-04, -3.6264e-04, -5.4237e-04],\n",
       "         [-1.3373e-04, -1.7735e-04, -5.0774e-04, -9.1114e-05, -6.5848e-05,\n",
       "          -5.5535e-05, -9.5008e-05, -2.6031e-04, -9.7414e-05, -1.4569e-04],\n",
       "         [-4.2961e-04, -5.6973e-04,  3.9784e-03, -2.9271e-04, -2.1154e-04,\n",
       "          -1.7841e-04, -3.0522e-04, -8.3625e-04, -3.1295e-04, -4.6805e-04],\n",
       "         [-9.3778e-04, -1.2437e-03,  1.9405e-02, -6.3894e-04, -4.6176e-04,\n",
       "           9.8842e-03, -6.6625e-04, -1.8254e-03, -6.8312e-04, -1.0217e-03],\n",
       "         [-2.9617e-04, -3.9277e-04, -1.1245e-03, -2.0179e-04, -1.4583e-04,\n",
       "          -1.2299e-04, -2.1041e-04, -5.7649e-04, -2.1574e-04, -3.2266e-04],\n",
       "         [-5.2180e-04, -6.9200e-04, -1.9812e-03, -3.5552e-04, -2.5693e-04,\n",
       "          -2.1669e-04, -3.7072e-04, -1.0157e-03, -3.8010e-04, -5.6849e-04],\n",
       "         [-3.1734e-04, -4.2084e-04, -1.0806e-03, -2.1621e-04, -1.5626e-04,\n",
       "          -1.3178e-04, -2.2545e-04, -6.1770e-04, -2.3116e-04, -3.4573e-04],\n",
       "         [-1.4278e-04, -1.8935e-04, -5.4210e-04, -9.7279e-05, -7.0304e-05,\n",
       "          -5.9293e-05, -1.0144e-04, -2.7792e-04, -1.0401e-04, -1.1163e-04],\n",
       "         [-4.1399e-04, -5.4901e-04, -6.3809e-04, -2.8206e-04,  1.0965e-04,\n",
       "          -1.7192e-04, -2.9412e-04, -8.0583e-04, -3.0156e-04, -4.5103e-04],\n",
       "         [-2.7353e-04, -3.6274e-04, -1.0385e-03, -1.8636e-04, -1.3468e-04,\n",
       "          -1.1359e-04, -1.9433e-04, -5.3243e-04, -1.9925e-04,  7.5407e-01]]),\n",
       " tensor([[-6.2438e-04, -2.5882e-04, -2.6010e-04, -3.0832e-04, -8.6990e-04,\n",
       "          -6.4761e-04, -1.1305e-04, -5.8632e-04, -2.9311e-04, -2.5225e-04],\n",
       "         [-8.2803e-04, -3.4324e-04, -3.4494e-04, -4.0888e-04, -1.1536e-03,\n",
       "          -8.5883e-04, -1.4992e-04, -7.7755e-04,  2.3899e-03, -3.3452e-04],\n",
       "         [-2.3706e-03, -1.2318e-04, -9.8067e-04, -1.1706e-03, -2.9752e-03,\n",
       "           7.0438e-03, -3.8068e-04, -2.2261e-03, -1.1129e-03, -9.4081e-04],\n",
       "         [-4.2541e-04, -1.7634e-04, -1.7722e-04, -2.1006e-04, -5.9269e-04,\n",
       "          -4.4123e-04, -7.7024e-05, -3.9947e-04, -1.9971e-04, -1.7186e-04],\n",
       "         [-3.0744e-04, -1.2744e-04, -1.2807e-04, -1.5181e-04, -4.2833e-04,\n",
       "          -3.1888e-04, -5.5666e-05, -2.8870e-04, -1.4433e-04, -1.2421e-04],\n",
       "         [-2.5929e-04, -1.0748e-04, -1.0802e-04, -1.2804e-04, -3.6125e-04,\n",
       "          -2.6894e-04, -4.6947e-05, -2.4348e-04, -1.2172e-04, -1.0475e-04],\n",
       "         [-4.4359e-04, -1.8388e-04, -1.8479e-04, -2.1904e-04, -6.1802e-04,\n",
       "          -4.6009e-04, -8.0317e-05,  3.5876e-04, -2.0824e-04, -1.7921e-04],\n",
       "         [-1.2154e-03, -5.0380e-04, -5.0630e-04, -6.0014e-04, -1.6351e-03,\n",
       "          -1.2606e-03, -2.2005e-04, -1.1376e-03, -5.7055e-04, -4.9100e-04],\n",
       "         [-4.5482e-04,  3.2150e-04, -1.8947e-04, -2.2459e-04, -6.3367e-04,\n",
       "          -4.7174e-04, -8.2351e-05, -4.2710e-04, -2.1352e-04, -1.8375e-04],\n",
       "         [-6.8024e-04,  6.9395e-03, -2.8338e-04, -3.3590e-04, -9.4773e-04,\n",
       "          -7.0555e-04, -1.2316e-04, -4.1326e-04, -3.1934e-04,  8.1220e-01]])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "pearson_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0000e+00,  9.0000e+00,  7.5331e-01],\n",
       "        [ 6.0000e+00,  7.0000e+00,  1.3372e-02],\n",
       "        [ 4.0000e+00,  0.0000e+00,  1.0548e-02],\n",
       "        [ 6.0000e+00,  2.0000e+00,  4.5319e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  3.1148e-03],\n",
       "        [ 8.0000e+00,  4.0000e+00,  1.4305e-03],\n",
       "        [ 1.0000e+00,  0.0000e+00,  1.2669e-03],\n",
       "        [ 2.0000e+00,  5.0000e+00,  9.7921e-04],\n",
       "        [ 7.0000e+00,  5.0000e+00,  9.1835e-05],\n",
       "        [ 3.0000e+00,  1.0000e+00,  5.9122e-05],\n",
       "        [ 7.0000e+00,  1.0000e+00, -7.5930e-05],\n",
       "        [ 7.0000e+00,  7.0000e+00, -8.1068e-05],\n",
       "        [ 5.0000e+00,  1.0000e+00, -9.4699e-05],\n",
       "        [ 5.0000e+00,  7.0000e+00, -1.0111e-04],\n",
       "        [ 2.0000e+00,  1.0000e+00, -1.0950e-04],\n",
       "        [ 3.0000e+00,  7.0000e+00, -1.1446e-04],\n",
       "        [ 2.0000e+00,  7.0000e+00, -1.1691e-04],\n",
       "        [ 7.0000e+00,  9.0000e+00, -1.5531e-04],\n",
       "        [ 9.0000e+00,  1.0000e+00, -1.6540e-04],\n",
       "        [ 7.0000e+00,  4.0000e+00, -1.6816e-04],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.7483e-04],\n",
       "        [ 9.0000e+00,  7.0000e+00, -1.7659e-04],\n",
       "        [ 7.0000e+00,  6.0000e+00, -1.8018e-04],\n",
       "        [ 1.0000e+00,  7.0000e+00, -1.8667e-04],\n",
       "        [ 5.0000e+00,  9.0000e+00, -1.9370e-04],\n",
       "        [ 0.0000e+00,  1.0000e+00, -1.9471e-04],\n",
       "        [ 0.0000e+00,  7.0000e+00, -2.0788e-04],\n",
       "        [ 5.0000e+00,  4.0000e+00, -2.0973e-04],\n",
       "        [ 3.0000e+00,  9.0000e+00, -2.1927e-04],\n",
       "        [ 2.0000e+00,  9.0000e+00, -2.2398e-04],\n",
       "        [ 5.0000e+00,  6.0000e+00, -2.2472e-04],\n",
       "        [ 6.0000e+00,  1.0000e+00, -2.3135e-04],\n",
       "        [ 7.0000e+00,  8.0000e+00, -2.3506e-04],\n",
       "        [ 3.0000e+00,  4.0000e+00, -2.3741e-04],\n",
       "        [ 2.0000e+00,  4.0000e+00, -2.4251e-04],\n",
       "        [ 7.0000e+00,  2.0000e+00, -2.4393e-04],\n",
       "        [ 3.0000e+00,  6.0000e+00, -2.5439e-04],\n",
       "        [ 2.0000e+00,  6.0000e+00, -2.5985e-04],\n",
       "        [ 7.0000e+00,  0.0000e+00, -2.8266e-04],\n",
       "        [ 5.0000e+00,  8.0000e+00, -2.9316e-04],\n",
       "        [ 5.0000e+00,  2.0000e+00, -3.0422e-04],\n",
       "        [ 3.0000e+00,  8.0000e+00, -3.3186e-04],\n",
       "        [ 2.0000e+00,  8.0000e+00, -3.3899e-04],\n",
       "        [ 3.0000e+00,  2.0000e+00, -3.4439e-04],\n",
       "        [ 2.0000e+00,  2.0000e+00, -3.5178e-04],\n",
       "        [ 5.0000e+00,  0.0000e+00, -3.5253e-04],\n",
       "        [ 1.0000e+00,  9.0000e+00, -3.5761e-04],\n",
       "        [ 9.0000e+00,  4.0000e+00, -3.6631e-04],\n",
       "        [ 5.0000e+00,  5.0000e+00, -3.6951e-04],\n",
       "        [ 1.0000e+00,  4.0000e+00, -3.8720e-04],\n",
       "        [ 9.0000e+00,  6.0000e+00, -3.9249e-04],\n",
       "        [ 0.0000e+00,  9.0000e+00, -3.9825e-04],\n",
       "        [ 3.0000e+00,  0.0000e+00, -3.9907e-04],\n",
       "        [ 2.0000e+00,  0.0000e+00, -4.0764e-04],\n",
       "        [ 8.0000e+00,  1.0000e+00, -4.0950e-04],\n",
       "        [ 1.0000e+00,  6.0000e+00, -4.1488e-04],\n",
       "        [ 3.0000e+00,  5.0000e+00, -4.1829e-04],\n",
       "        [ 0.0000e+00,  4.0000e+00, -4.3121e-04],\n",
       "        [ 8.0000e+00,  0.0000e+00, -4.3671e-04],\n",
       "        [ 9.0000e+00,  0.0000e+00, -4.4885e-04],\n",
       "        [ 0.0000e+00,  6.0000e+00, -4.6204e-04],\n",
       "        [ 6.0000e+00,  9.0000e+00, -4.7319e-04],\n",
       "        [ 9.0000e+00,  8.0000e+00, -5.1203e-04],\n",
       "        [ 6.0000e+00,  4.0000e+00, -5.1236e-04],\n",
       "        [ 9.0000e+00,  2.0000e+00, -5.3136e-04],\n",
       "        [ 7.0000e+00,  3.0000e+00, -5.3246e-04],\n",
       "        [ 4.0000e+00,  1.0000e+00, -5.3831e-04],\n",
       "        [ 1.0000e+00,  8.0000e+00, -5.4124e-04],\n",
       "        [ 6.0000e+00,  6.0000e+00, -5.4898e-04],\n",
       "        [ 8.0000e+00,  7.0000e+00, -5.5037e-04],\n",
       "        [ 1.0000e+00,  2.0000e+00, -5.6166e-04],\n",
       "        [ 4.0000e+00,  7.0000e+00, -5.7474e-04],\n",
       "        [ 0.0000e+00,  8.0000e+00, -6.0275e-04],\n",
       "        [ 0.0000e+00,  2.0000e+00, -6.2550e-04],\n",
       "        [ 8.0000e+00,  9.0000e+00, -6.2903e-04],\n",
       "        [ 8.0000e+00,  5.0000e+00, -6.4137e-04],\n",
       "        [ 9.0000e+00,  5.0000e+00, -6.4538e-04],\n",
       "        [ 5.0000e+00,  3.0000e+00, -6.6408e-04],\n",
       "        [ 6.0000e+00,  8.0000e+00, -6.7742e-04],\n",
       "        [ 1.0000e+00,  5.0000e+00, -6.8220e-04],\n",
       "        [ 3.0000e+00,  3.0000e+00, -7.5175e-04],\n",
       "        [ 0.0000e+00,  5.0000e+00, -7.5973e-04],\n",
       "        [ 2.0000e+00,  3.0000e+00, -7.5998e-04],\n",
       "        [ 6.0000e+00,  0.0000e+00, -8.6122e-04],\n",
       "        [ 6.0000e+00,  5.0000e+00, -9.0270e-04],\n",
       "        [ 8.0000e+00,  2.0000e+00, -9.3975e-04],\n",
       "        [ 4.0000e+00,  9.0000e+00, -1.1011e-03],\n",
       "        [ 9.0000e+00,  3.0000e+00, -1.1532e-03],\n",
       "        [ 4.0000e+00,  4.0000e+00, -1.1922e-03],\n",
       "        [ 8.0000e+00,  6.0000e+00, -1.2233e-03],\n",
       "        [ 1.0000e+00,  3.0000e+00, -1.2260e-03],\n",
       "        [ 4.0000e+00,  6.0000e+00, -1.2774e-03],\n",
       "        [ 0.0000e+00,  3.0000e+00, -1.3654e-03],\n",
       "        [ 8.0000e+00,  8.0000e+00, -1.3794e-03],\n",
       "        [ 4.0000e+00,  5.0000e+00, -1.5441e-03],\n",
       "        [ 6.0000e+00,  3.0000e+00, -1.6223e-03],\n",
       "        [ 4.0000e+00,  8.0000e+00, -1.6665e-03],\n",
       "        [ 4.0000e+00,  2.0000e+00, -1.7294e-03],\n",
       "        [ 8.0000e+00,  3.0000e+00, -3.6149e-03],\n",
       "        [ 4.0000e+00,  3.0000e+00, -3.7555e-03],\n",
       "        [ 9.0000e+00,  9.0000e+00,  7.5407e-01],\n",
       "        [ 3.0000e+00,  2.0000e+00,  1.9405e-02],\n",
       "        [ 3.0000e+00,  5.0000e+00,  9.8842e-03],\n",
       "        [ 2.0000e+00,  2.0000e+00,  3.9784e-03],\n",
       "        [ 0.0000e+00,  6.0000e+00,  7.3718e-04],\n",
       "        [ 8.0000e+00,  4.0000e+00,  1.0965e-04],\n",
       "        [ 1.0000e+00,  5.0000e+00, -5.5535e-05],\n",
       "        [ 7.0000e+00,  5.0000e+00, -5.9293e-05],\n",
       "        [ 1.0000e+00,  4.0000e+00, -6.5848e-05],\n",
       "        [ 7.0000e+00,  4.0000e+00, -7.0304e-05],\n",
       "        [ 1.0000e+00,  3.0000e+00, -9.1114e-05],\n",
       "        [ 1.0000e+00,  6.0000e+00, -9.5008e-05],\n",
       "        [ 7.0000e+00,  3.0000e+00, -9.7279e-05],\n",
       "        [ 1.0000e+00,  8.0000e+00, -9.7414e-05],\n",
       "        [ 7.0000e+00,  6.0000e+00, -1.0144e-04],\n",
       "        [ 7.0000e+00,  8.0000e+00, -1.0401e-04],\n",
       "        [ 7.0000e+00,  9.0000e+00, -1.1163e-04],\n",
       "        [ 9.0000e+00,  5.0000e+00, -1.1359e-04],\n",
       "        [ 4.0000e+00,  5.0000e+00, -1.2299e-04],\n",
       "        [ 6.0000e+00,  5.0000e+00, -1.3178e-04],\n",
       "        [ 1.0000e+00,  0.0000e+00, -1.3373e-04],\n",
       "        [ 9.0000e+00,  4.0000e+00, -1.3468e-04],\n",
       "        [ 7.0000e+00,  0.0000e+00, -1.4278e-04],\n",
       "        [ 1.0000e+00,  9.0000e+00, -1.4569e-04],\n",
       "        [ 4.0000e+00,  4.0000e+00, -1.4583e-04],\n",
       "        [ 6.0000e+00,  4.0000e+00, -1.5626e-04],\n",
       "        [ 8.0000e+00,  5.0000e+00, -1.7192e-04],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.7735e-04],\n",
       "        [ 2.0000e+00,  5.0000e+00, -1.7841e-04],\n",
       "        [ 9.0000e+00,  3.0000e+00, -1.8636e-04],\n",
       "        [ 7.0000e+00,  1.0000e+00, -1.8935e-04],\n",
       "        [ 9.0000e+00,  6.0000e+00, -1.9433e-04],\n",
       "        [ 9.0000e+00,  8.0000e+00, -1.9925e-04],\n",
       "        [ 4.0000e+00,  3.0000e+00, -2.0179e-04],\n",
       "        [ 0.0000e+00,  5.0000e+00, -2.0674e-04],\n",
       "        [ 4.0000e+00,  6.0000e+00, -2.1041e-04],\n",
       "        [ 2.0000e+00,  4.0000e+00, -2.1154e-04],\n",
       "        [ 4.0000e+00,  8.0000e+00, -2.1574e-04],\n",
       "        [ 6.0000e+00,  3.0000e+00, -2.1621e-04],\n",
       "        [ 5.0000e+00,  5.0000e+00, -2.1669e-04],\n",
       "        [ 6.0000e+00,  6.0000e+00, -2.2545e-04],\n",
       "        [ 6.0000e+00,  8.0000e+00, -2.3116e-04],\n",
       "        [ 0.0000e+00,  4.0000e+00, -2.4513e-04],\n",
       "        [ 5.0000e+00,  4.0000e+00, -2.5693e-04],\n",
       "        [ 1.0000e+00,  7.0000e+00, -2.6031e-04],\n",
       "        [ 9.0000e+00,  0.0000e+00, -2.7353e-04],\n",
       "        [ 7.0000e+00,  7.0000e+00, -2.7792e-04],\n",
       "        [ 8.0000e+00,  3.0000e+00, -2.8206e-04],\n",
       "        [ 2.0000e+00,  3.0000e+00, -2.9271e-04],\n",
       "        [ 8.0000e+00,  6.0000e+00, -2.9412e-04],\n",
       "        [ 4.0000e+00,  0.0000e+00, -2.9617e-04],\n",
       "        [ 8.0000e+00,  8.0000e+00, -3.0156e-04],\n",
       "        [ 2.0000e+00,  6.0000e+00, -3.0522e-04],\n",
       "        [ 2.0000e+00,  8.0000e+00, -3.1295e-04],\n",
       "        [ 6.0000e+00,  0.0000e+00, -3.1734e-04],\n",
       "        [ 4.0000e+00,  9.0000e+00, -3.2266e-04],\n",
       "        [ 0.0000e+00,  3.0000e+00, -3.3918e-04],\n",
       "        [ 6.0000e+00,  9.0000e+00, -3.4573e-04],\n",
       "        [ 5.0000e+00,  3.0000e+00, -3.5552e-04],\n",
       "        [ 0.0000e+00,  8.0000e+00, -3.6264e-04],\n",
       "        [ 9.0000e+00,  1.0000e+00, -3.6274e-04],\n",
       "        [ 5.0000e+00,  6.0000e+00, -3.7072e-04],\n",
       "        [ 5.0000e+00,  8.0000e+00, -3.8010e-04],\n",
       "        [ 4.0000e+00,  1.0000e+00, -3.9277e-04],\n",
       "        [ 8.0000e+00,  0.0000e+00, -4.1399e-04],\n",
       "        [ 6.0000e+00,  1.0000e+00, -4.2084e-04],\n",
       "        [ 2.0000e+00,  0.0000e+00, -4.2961e-04],\n",
       "        [ 8.0000e+00,  9.0000e+00, -4.5103e-04],\n",
       "        [ 3.0000e+00,  4.0000e+00, -4.6176e-04],\n",
       "        [ 2.0000e+00,  9.0000e+00, -4.6805e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00, -4.9782e-04],\n",
       "        [ 1.0000e+00,  2.0000e+00, -5.0774e-04],\n",
       "        [ 5.0000e+00,  0.0000e+00, -5.2180e-04],\n",
       "        [ 9.0000e+00,  7.0000e+00, -5.3243e-04],\n",
       "        [ 7.0000e+00,  2.0000e+00, -5.4210e-04],\n",
       "        [ 0.0000e+00,  9.0000e+00, -5.4237e-04],\n",
       "        [ 8.0000e+00,  1.0000e+00, -5.4901e-04],\n",
       "        [ 5.0000e+00,  9.0000e+00, -5.6849e-04],\n",
       "        [ 2.0000e+00,  1.0000e+00, -5.6973e-04],\n",
       "        [ 4.0000e+00,  7.0000e+00, -5.7649e-04],\n",
       "        [ 6.0000e+00,  7.0000e+00, -6.1770e-04],\n",
       "        [ 8.0000e+00,  2.0000e+00, -6.3809e-04],\n",
       "        [ 3.0000e+00,  3.0000e+00, -6.3894e-04],\n",
       "        [ 0.0000e+00,  1.0000e+00, -6.6020e-04],\n",
       "        [ 3.0000e+00,  6.0000e+00, -6.6625e-04],\n",
       "        [ 3.0000e+00,  8.0000e+00, -6.8312e-04],\n",
       "        [ 5.0000e+00,  1.0000e+00, -6.9200e-04],\n",
       "        [ 8.0000e+00,  7.0000e+00, -8.0583e-04],\n",
       "        [ 2.0000e+00,  7.0000e+00, -8.3625e-04],\n",
       "        [ 0.0000e+00,  2.0000e+00, -8.6356e-04],\n",
       "        [ 3.0000e+00,  0.0000e+00, -9.3778e-04],\n",
       "        [ 0.0000e+00,  7.0000e+00, -9.6903e-04],\n",
       "        [ 5.0000e+00,  7.0000e+00, -1.0157e-03],\n",
       "        [ 3.0000e+00,  9.0000e+00, -1.0217e-03],\n",
       "        [ 9.0000e+00,  2.0000e+00, -1.0385e-03],\n",
       "        [ 6.0000e+00,  2.0000e+00, -1.0806e-03],\n",
       "        [ 4.0000e+00,  2.0000e+00, -1.1245e-03],\n",
       "        [ 3.0000e+00,  1.0000e+00, -1.2437e-03],\n",
       "        [ 3.0000e+00,  7.0000e+00, -1.8254e-03],\n",
       "        [ 5.0000e+00,  2.0000e+00, -1.9812e-03],\n",
       "        [ 9.0000e+00,  9.0000e+00,  8.1220e-01],\n",
       "        [ 2.0000e+00,  5.0000e+00,  7.0438e-03],\n",
       "        [ 9.0000e+00,  1.0000e+00,  6.9395e-03],\n",
       "        [ 1.0000e+00,  8.0000e+00,  2.3899e-03],\n",
       "        [ 6.0000e+00,  7.0000e+00,  3.5876e-04],\n",
       "        [ 8.0000e+00,  1.0000e+00,  3.2150e-04],\n",
       "        [ 5.0000e+00,  6.0000e+00, -4.6947e-05],\n",
       "        [ 4.0000e+00,  6.0000e+00, -5.5666e-05],\n",
       "        [ 3.0000e+00,  6.0000e+00, -7.7024e-05],\n",
       "        [ 6.0000e+00,  6.0000e+00, -8.0317e-05],\n",
       "        [ 8.0000e+00,  6.0000e+00, -8.2351e-05],\n",
       "        [ 5.0000e+00,  9.0000e+00, -1.0475e-04],\n",
       "        [ 5.0000e+00,  1.0000e+00, -1.0748e-04],\n",
       "        [ 5.0000e+00,  2.0000e+00, -1.0802e-04],\n",
       "        [ 0.0000e+00,  6.0000e+00, -1.1305e-04],\n",
       "        [ 5.0000e+00,  8.0000e+00, -1.2172e-04],\n",
       "        [ 9.0000e+00,  6.0000e+00, -1.2316e-04],\n",
       "        [ 2.0000e+00,  1.0000e+00, -1.2318e-04],\n",
       "        [ 4.0000e+00,  9.0000e+00, -1.2421e-04],\n",
       "        [ 4.0000e+00,  1.0000e+00, -1.2744e-04],\n",
       "        [ 5.0000e+00,  3.0000e+00, -1.2804e-04],\n",
       "        [ 4.0000e+00,  2.0000e+00, -1.2807e-04],\n",
       "        [ 4.0000e+00,  8.0000e+00, -1.4433e-04],\n",
       "        [ 1.0000e+00,  6.0000e+00, -1.4992e-04],\n",
       "        [ 4.0000e+00,  3.0000e+00, -1.5181e-04],\n",
       "        [ 3.0000e+00,  9.0000e+00, -1.7186e-04],\n",
       "        [ 3.0000e+00,  1.0000e+00, -1.7634e-04],\n",
       "        [ 3.0000e+00,  2.0000e+00, -1.7722e-04],\n",
       "        [ 6.0000e+00,  9.0000e+00, -1.7921e-04],\n",
       "        [ 8.0000e+00,  9.0000e+00, -1.8375e-04],\n",
       "        [ 6.0000e+00,  1.0000e+00, -1.8388e-04],\n",
       "        [ 6.0000e+00,  2.0000e+00, -1.8479e-04],\n",
       "        [ 8.0000e+00,  2.0000e+00, -1.8947e-04],\n",
       "        [ 3.0000e+00,  8.0000e+00, -1.9971e-04],\n",
       "        [ 6.0000e+00,  8.0000e+00, -2.0824e-04],\n",
       "        [ 3.0000e+00,  3.0000e+00, -2.1006e-04],\n",
       "        [ 8.0000e+00,  8.0000e+00, -2.1352e-04],\n",
       "        [ 6.0000e+00,  3.0000e+00, -2.1904e-04],\n",
       "        [ 7.0000e+00,  6.0000e+00, -2.2005e-04],\n",
       "        [ 8.0000e+00,  3.0000e+00, -2.2459e-04],\n",
       "        [ 5.0000e+00,  7.0000e+00, -2.4348e-04],\n",
       "        [ 0.0000e+00,  9.0000e+00, -2.5225e-04],\n",
       "        [ 0.0000e+00,  1.0000e+00, -2.5882e-04],\n",
       "        [ 5.0000e+00,  0.0000e+00, -2.5929e-04],\n",
       "        [ 0.0000e+00,  2.0000e+00, -2.6010e-04],\n",
       "        [ 5.0000e+00,  5.0000e+00, -2.6894e-04],\n",
       "        [ 9.0000e+00,  2.0000e+00, -2.8338e-04],\n",
       "        [ 4.0000e+00,  7.0000e+00, -2.8870e-04],\n",
       "        [ 0.0000e+00,  8.0000e+00, -2.9311e-04],\n",
       "        [ 4.0000e+00,  0.0000e+00, -3.0744e-04],\n",
       "        [ 0.0000e+00,  3.0000e+00, -3.0832e-04],\n",
       "        [ 4.0000e+00,  5.0000e+00, -3.1888e-04],\n",
       "        [ 9.0000e+00,  8.0000e+00, -3.1934e-04],\n",
       "        [ 1.0000e+00,  9.0000e+00, -3.3452e-04],\n",
       "        [ 9.0000e+00,  3.0000e+00, -3.3590e-04],\n",
       "        [ 1.0000e+00,  1.0000e+00, -3.4324e-04],\n",
       "        [ 1.0000e+00,  2.0000e+00, -3.4494e-04],\n",
       "        [ 5.0000e+00,  4.0000e+00, -3.6125e-04],\n",
       "        [ 2.0000e+00,  6.0000e+00, -3.8068e-04],\n",
       "        [ 3.0000e+00,  7.0000e+00, -3.9947e-04],\n",
       "        [ 1.0000e+00,  3.0000e+00, -4.0888e-04],\n",
       "        [ 9.0000e+00,  7.0000e+00, -4.1326e-04],\n",
       "        [ 3.0000e+00,  0.0000e+00, -4.2541e-04],\n",
       "        [ 8.0000e+00,  7.0000e+00, -4.2710e-04],\n",
       "        [ 4.0000e+00,  4.0000e+00, -4.2833e-04],\n",
       "        [ 3.0000e+00,  5.0000e+00, -4.4123e-04],\n",
       "        [ 6.0000e+00,  0.0000e+00, -4.4359e-04],\n",
       "        [ 8.0000e+00,  0.0000e+00, -4.5482e-04],\n",
       "        [ 6.0000e+00,  5.0000e+00, -4.6009e-04],\n",
       "        [ 8.0000e+00,  5.0000e+00, -4.7174e-04],\n",
       "        [ 7.0000e+00,  9.0000e+00, -4.9100e-04],\n",
       "        [ 7.0000e+00,  1.0000e+00, -5.0380e-04],\n",
       "        [ 7.0000e+00,  2.0000e+00, -5.0630e-04],\n",
       "        [ 7.0000e+00,  8.0000e+00, -5.7055e-04],\n",
       "        [ 0.0000e+00,  7.0000e+00, -5.8632e-04],\n",
       "        [ 3.0000e+00,  4.0000e+00, -5.9269e-04],\n",
       "        [ 7.0000e+00,  3.0000e+00, -6.0014e-04],\n",
       "        [ 6.0000e+00,  4.0000e+00, -6.1802e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00, -6.2438e-04],\n",
       "        [ 8.0000e+00,  4.0000e+00, -6.3367e-04],\n",
       "        [ 0.0000e+00,  5.0000e+00, -6.4761e-04],\n",
       "        [ 9.0000e+00,  0.0000e+00, -6.8024e-04],\n",
       "        [ 9.0000e+00,  5.0000e+00, -7.0555e-04],\n",
       "        [ 1.0000e+00,  7.0000e+00, -7.7755e-04],\n",
       "        [ 1.0000e+00,  0.0000e+00, -8.2803e-04],\n",
       "        [ 1.0000e+00,  5.0000e+00, -8.5883e-04],\n",
       "        [ 0.0000e+00,  4.0000e+00, -8.6990e-04],\n",
       "        [ 2.0000e+00,  9.0000e+00, -9.4081e-04],\n",
       "        [ 9.0000e+00,  4.0000e+00, -9.4773e-04],\n",
       "        [ 2.0000e+00,  2.0000e+00, -9.8067e-04],\n",
       "        [ 2.0000e+00,  8.0000e+00, -1.1129e-03],\n",
       "        [ 7.0000e+00,  7.0000e+00, -1.1376e-03],\n",
       "        [ 1.0000e+00,  4.0000e+00, -1.1536e-03],\n",
       "        [ 2.0000e+00,  3.0000e+00, -1.1706e-03],\n",
       "        [ 7.0000e+00,  0.0000e+00, -1.2154e-03],\n",
       "        [ 7.0000e+00,  5.0000e+00, -1.2606e-03],\n",
       "        [ 7.0000e+00,  4.0000e+00, -1.6351e-03],\n",
       "        [ 2.0000e+00,  7.0000e+00, -2.2261e-03],\n",
       "        [ 2.0000e+00,  0.0000e+00, -2.3706e-03],\n",
       "        [ 2.0000e+00,  4.0000e+00, -2.9752e-03]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058958083391189575"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(sae_activations[agg_conf.layer_1.layer_idx, agg_conf.layer_1.feature_idxes]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2438e-04, -2.5882e-04, -2.6010e-04, -3.0832e-04, -8.6990e-04,\n",
       "         -6.4761e-04, -1.1305e-04, -5.8632e-04, -2.9311e-04, -2.5225e-04],\n",
       "        [-8.2803e-04, -3.4324e-04, -3.4494e-04, -4.0888e-04, -1.1536e-03,\n",
       "         -8.5883e-04, -1.4992e-04, -7.7755e-04,  2.3899e-03, -3.3452e-04],\n",
       "        [-2.3706e-03, -1.2318e-04, -9.8067e-04, -1.1706e-03, -2.9752e-03,\n",
       "          7.0438e-03, -3.8068e-04, -2.2261e-03, -1.1129e-03, -9.4081e-04],\n",
       "        [-4.2541e-04, -1.7634e-04, -1.7722e-04, -2.1006e-04, -5.9269e-04,\n",
       "         -4.4123e-04, -7.7024e-05, -3.9947e-04, -1.9971e-04, -1.7186e-04],\n",
       "        [-3.0744e-04, -1.2744e-04, -1.2807e-04, -1.5181e-04, -4.2833e-04,\n",
       "         -3.1888e-04, -5.5666e-05, -2.8870e-04, -1.4433e-04, -1.2421e-04],\n",
       "        [-2.5929e-04, -1.0748e-04, -1.0802e-04, -1.2804e-04, -3.6125e-04,\n",
       "         -2.6894e-04, -4.6947e-05, -2.4348e-04, -1.2172e-04, -1.0475e-04],\n",
       "        [-4.4359e-04, -1.8388e-04, -1.8479e-04, -2.1904e-04, -6.1802e-04,\n",
       "         -4.6009e-04, -8.0317e-05,  3.5876e-04, -2.0824e-04, -1.7921e-04],\n",
       "        [-1.2154e-03, -5.0380e-04, -5.0630e-04, -6.0014e-04, -1.6351e-03,\n",
       "         -1.2606e-03, -2.2005e-04, -1.1376e-03, -5.7055e-04, -4.9100e-04],\n",
       "        [-4.5482e-04,  3.2150e-04, -1.8947e-04, -2.2459e-04, -6.3367e-04,\n",
       "         -4.7174e-04, -8.2351e-05, -4.2710e-04, -2.1352e-04, -1.8375e-04],\n",
       "        [-6.8024e-04,  6.9395e-03, -2.8338e-04, -3.3590e-04, -9.4773e-04,\n",
       "         -7.0555e-04, -1.2316e-04, -4.1326e-04, -3.1934e-04,  8.1220e-01]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_correlations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uhhh\n",
    "it was originally `1.0054`..hmm...okay...honestly...I need to just forget about `pearson_0_1.pt` and just recompute it myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work on multiple feature pairs within a single layer pair\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to work across multiple layer pairs\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
